{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d35659-f5a0-416a-b180-8a45eef4aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21175feb-083b-4c6f-8da9-5aec86ff5d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30005, 2048)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\", bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\").cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d248b9-ca25-40c8-be79-d748ddda10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(f\"\"\"<|startoftext|>\n",
    "            질문에 대답하세요.\\n\"\"\" + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6205e0-7b72-4896-a778-a82046141e94",
   "metadata": {},
   "source": [
    "# 콜센터 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddd4f6a-c023-4fc4-990b-452bef66b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('./call_data/민원(콜센터) 질의응답_질병관리본부_약품식품_Training.json', encoding='UTF-8')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf107d5-710d-4a93-b9f5-1c489aa12246",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "qna = []\n",
    "for i in range(len(data)):\n",
    "    if data[i]['고객질문(요청)']:\n",
    "        text += \"Q: \" + data[i]['고객질문(요청)'] + \"\\n \"\n",
    "        if len(data[i + 1]['상담사답변']) > 10:\n",
    "            text += \"A: \" + data[i + 1]['상담사답변'] \n",
    "            qna += [text]\n",
    "    if data[i]['상담사질문(요청)']:\n",
    "        text += \"Q: \" + data[i]['상담사질문(요청)'] + \"\\n \"\n",
    "        if len(data[i + 1]['고객답변']) > 10:\n",
    "            text += \"A: \" + data[i + 1]['고객답변']\n",
    "            qna += [text]\n",
    "    text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fca2ba-f5dc-4a51-bf0a-0fa64cf491f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna = qna[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42551096-4422-4f86-aada-d8cc329dae37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c55bea-02e3-4560-bb91-e7f0ed6d3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_series = pd.Series(qna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e55f33a-c473-4feb-a784-4bf2797b7830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 52\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(tokenizer.encode(e)) for e in qna_series])\n",
    "print(\"Max length: {}\".format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8b317f-c7fa-46ad-afa0-3bfd26f8b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(qna_series, tokenizer, max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33a3a06-69be-41c8-940d-3817c342563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch \n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189cd5b-380f-4a32-8f22-5580b0454161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\GPU3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [419/450 01:38 < 00:07, 4.22 it/s, Epoch 4.64/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=5, logging_steps=1000,\n",
    "                                  save_strategy=IntervalStrategy.NO,\n",
    "                                  per_device_train_batch_size=2, per_device_eval_batch_size=2,\n",
    "                                  warmup_steps=100, weight_decay=0.01, logging_dir='./logs')\n",
    "Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e19fcf-771e-4b85-b1db-1926b6e69888",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                # bos_token='<|startoftext|>',\n",
    "                                # eos_token='<|endoftext|>', pad_token='<|pad|>',\n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aed37c-ea54-4180-baec-2e9225e90e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {} \\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094f4051-6e2a-4c7d-9c9b-a936e75bd6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0962610663453b908fa8937f36a785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93f33b35-2ed1-4325-b609-9afdcfe0c69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eunyounglee/polyglot_kr_0330/commit/fb76ef1bbd321c77dca27c7e94ca758d0baab3a1', commit_message='Upload tokenizer', commit_description='', oid='fb76ef1bbd321c77dca27c7e94ca758d0baab3a1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"eunyounglee/polyglot_kr_0330\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5e82b-e78b-4561-ae33-b9ca50ced223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:\\Users\\User\\AppData\\Local\\Temp\\tmpv5tcm9w5\\config.json\n",
      "Model weights saved in C:\\Users\\User\\AppData\\Local\\Temp\\tmpv5tcm9w5\\pytorch_model.bin\n",
      "Uploading the following files to eunyounglee/polyglot_kr_0323: config.json,pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# model.push_to_hub(\"eunyounglee/polyglot_kr_0323\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abd60b87-37f0-449c-ac5c-9c960f97e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./polyglot_ko_0330_2/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbba9-af66-4952-9c68-93253c414e37",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98459b19-54e1-412b-ab56-fd959d78e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40b17838-6db8-4955-a744-9288a8bdc91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q: 코로나 검사결과는 언제 나와요? \\n A: 6시간에서 하루 정도 소요됩니다. ',\n",
       " 'Q: 결과는 어떻게 알 수 있어요? \\n A: 문자로 양성, 음성 여부를 알려줍니다. . ',\n",
       " 'Q: 하루가 지나도 연락이 안오면 어떻게 해요?\\n A: 검사 받으신 진료소로 전화주셔서 확인하시면 됩니다. ',\n",
       " 'Q: 친구 아버지가 돌아가셨는데 장례식장에 가도 돼요? \\n A: 가셔도 되지만, 가급적 사람들이 많이 모이는 곳은 피하시는 것이 좋습니다. ',\n",
       " 'Q: 장례식장에 마스크 안끼고 가도 돼요?\\n A: 마스크는 꼭 착용하셔야 합니다. ',\n",
       " 'Q: 장례식장에서 밥 먹을 수 있어요?\\n A: 네, 한 방향으로 앉거나, 지그재그로 앉은 상태에서 식사 가능하십니다.',\n",
       " 'Q: 장례식장에서도 거리두기 해야 돼요?\\n A: 2m 거리두기를 원칙으로 하고 있습니다.',\n",
       " 'Q: 열이 있는데 가도 될까요?\\n A: 열이 있으시면 되도록 가지 않기를 권장합니다. ',\n",
       " 'Q: 어떤 장소에서 의무적으로 마스크를 써야하나요?\\n A: 버스,지하철과 같은 대중교통 이용할 떄와 병원 요양시설,집회등에서는 마스크를 의무적으로 착용핫야 합니다',\n",
       " 'Q: 그럼 1단계 일땐 어떻게 적용되나요?\\n A: 1단계 일땐 고위험 시설에서 의무적용됩니다']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24952809-9859-40b9-803c-12be7d601da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"질문에 대답하세요.\\nQ: 열이 있는데 장례식장에 가도 되나요?\" #@param {type:\"string\"}\n",
    "response_min_chars =  50#@param {type:\"integer\"}\n",
    "response_max_chars =  100#@param {type:\"integer\"}\n",
    "\n",
    "response_3 = generator(prompt3, do_sample=True, min_length=response_min_chars, \n",
    "                       max_length=response_max_chars,\n",
    "                       clean_up_tokenization_spaces=True,\n",
    "                       return_full_text=True)\n",
    "out3_dict = response_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a756a37-f6c5-418b-a559-59192b16c09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': '질문에 대답하세요.\\nQ: 열이 있는데 장례식장에 가도 되나요?\\n A: 열이 있으시면 되도록 가지 않기를 권장합니다.   질문에 대답하세요.\\nQ: 친척들과 노래방 갈 수 있어요? \\n A: 비수도권 지역이라면 노래방 출입은 가능하십니다만 방역수칙을 잘 지키셔야됩니다.  '}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3888f6f6-9e26-4d05-902d-1631161c3e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 코로나 검사결과는 언제 나와?  A: 6시간에서 하루 정도 소요됩니다.  시에서는 마스크 구매할때도 방역수칙을 잘 지키셔야됩니다 http://pf.kakao.com/_JCMKR/chat이르면 추석 전 사우나 갈 수 있는거\n"
     ]
    }
   ],
   "source": [
    "result = out3_dict['generated_text'].split(\"\\n\")\n",
    "print(result[1], result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698cfd8c-5552-40aa-b008-f1ba486d3b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c97a158e0e459b8beec50f250617c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f903ea87444b9be623ff2ce22feb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a83aabee37f4289ba431012e59a590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763d8eaf0475493cab82f0be61488f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483e37679224c1c9e9430409eac8902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2023 15:42:33 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyGeneration\n",
    "\n",
    "happy_gen = HappyGeneration(model_type = \"GPT-NEO\", model_name = \"eunyounglee/polyglot_kr_0330\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97c41570-97ad-4239-a75b-a97905b0116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = happy_gen.generate_text(\"계량기에서 소리가 나는데 교환이 가능할까요?\")\n",
    "\n",
    "# print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f6511e6-f307-4a49-91ef-c741f74eb5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyGeneration, GENSettings\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "greedy_settings = GENSettings(max_length=100)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"코로나에 걸리면 죽나요?\",\n",
    "    args=greedy_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f59e957-49ae-4864-b194-a8cb0fddb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = output_greedy.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "622a3ced-8c71-4d88-afa5-c3d247baf85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'코로나19에 걸린다고 해서 모두 폐가 망가지는 것은 아니며 그렇게 심하지않은 것으로 나타났습니다. '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.replace(\"\\n\", \"\")[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df218525-bebb-4fed-adc9-7ebe1c951574",
   "metadata": {},
   "source": [
    "## 질문 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cc633-47da-4dfc-8f11-0c57d33b2d00",
   "metadata": {},
   "source": [
    "고객번호란 무엇이고, 어떻게 확인할 수 있나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c0c44-e688-432d-b01b-ff9f1ee73955",
   "metadata": {},
   "source": [
    "○ 고객번호는 전기사용 계약단위별로 부여되는 고유번호입니다. 전기요금 청구서 우측 상단에서 확인하실 수 있으며 이를 통해 사이버지점에서 전기 사용 관련 유용한 정보를 제공받으실 수 있습니다.○ 만약 고객님의 전기요금이 아파트 관리비 청구서에 포함되어 청구된다면 아파트 전체를 하나의 고객번호로 한전과 계약한 경우로 이때, 세대별로 부여되는 고객번호는 없습니다. 다만, 대가족요금, 복지할인요금 등은 아파트 전체의 고객번호로 신청이 가능하오니 아파트 관리사무소나 한전 고객센터(국번없이 123)로 아파트 고객번호를 문의하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04a3875c-36b4-4003-ab5b-289ff016e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "greedy_settings = GENSettings(max_length=200)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"고객번호가 무엇인가요?\",\n",
    "    args=greedy_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b322c8b3-bcc3-419f-9d3d-a98f8f2960de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전기사용계약단위별로 부여되는 고유번호입니다. 전기요금 청구서 우측 상단에서 확인하실 수 있으며 이를 통해 사이버지점에서 전기 사용 관련 유용한 정보를 제공받으실 수 있습니다.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy.text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128f0d9-6e3c-48e4-a9d6-4967b466fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1fecbbc-5b82-4f4a-9e5b-ac807b1428ad",
   "metadata": {},
   "source": [
    "변전소는 무엇이며 무슨 기능을 하는 곳입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379610b-9207-42df-908f-9a04737ac32a",
   "metadata": {},
   "source": [
    "변전소라함은 구외로부터 보내오는 66kV, 154kV, 345kV 등의 송전전압을 구내에 시설한 변압기등에 의하여 전압을 높이거나 낮추어 다시 구외로 보내는 장소로서 한전이 소유하는 것을 말합니다.전압을 변환하는 방법은 전자유도작용의 원리를 이용하여 만든 변압기를 사용하는데 변압기에는철심에 코일을 감아서 만든 것입니다. 또한 변전소는 변압기 이외에 차단기, 단로기, 모선 등으로구성되어 있습니다. 그러나 전압을 낮춰서 자기의 공장 또는 건물내에만 쓰는 경우는 변전소라하지 않고 수전실, 변전실 등의 명칭을 쓰고 있습니다."
   ]
  },
  {
   "attachments": {
    "2bb17e71-16f3-4698-b3b6-c126b5ecd49b.png": {
     "image/png": "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"
    },
    "59e9a5ed-011b-4040-866a-5a90c5dae580.png": {
     "image/png": "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"
    },
    "af2d1240-1cfb-4207-bd43-8a5424043267.png": {
     "image/png": "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"
    },
    "c5fc8291-737f-4736-94f5-586eafdd2485.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5ab93000-eb04-4057-b669-1b4dbb8a12b8",
   "metadata": {},
   "source": [
    "전기요금을 미리 계산해 볼 수 있나요.\t`전기요금계산기(새창열림)`href=`http://cyber.kepco.co.kr/ckepco/front/jsp/CY/J/A/CYJAPP000.jsp`>사이버지점전기요금계산기에서 가능합니다.\n",
    "![image.png](attachment:59e9a5ed-011b-4040-866a-5a90c5dae580.png)![image.png](attachment:af2d1240-1cfb-4207-bd43-8a5424043267.png)![image.png](attachment:2bb17e71-16f3-4698-b3b6-c126b5ecd49b.png)![image.png](attachment:c5fc8291-737f-4736-94f5-586eafdd2485.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b5765f7-03a0-4ec2-8353-cf373b358702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "greedy_settings = GENSettings(max_length=300)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"전기요금을 미리 계산해 보고 싶습니다.\",\n",
    "    args=greedy_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ba8ea64-b96c-4d5f-ba6e-d7cffeb8bf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'`전기요금계산기(새창열림)`href=`http://cyber.kepco.co.kr/ckepco/front/jsp/CY/F/A/CYFAPP0016001.jsp`>사이버지점전기요금계산기에서 가능합니다.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy.text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756c4c2-bcf2-4aa0-9c91-5788d76de8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf097ecd-9481-471a-a31b-369b5d3b0a8a",
   "metadata": {},
   "source": [
    "전기요금을 미납한 경우 연체료율은 몇%입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21663a93-d348-44a7-bf81-cb05f1b8856a",
   "metadata": {},
   "source": [
    "전기요금을 납기일까지 납부하지 아니한 경우에는 연체료를 부담하셔야 합니다. 연체료율은 기간별로 차등적용하며 일수계산하여 부과하고 있습니다. - 전기요금 연체료 : 처음 1개월 미납시 1.5%로 일수계산,다음 1개월 미납시 1.5%로 일수계산 - TV수신료에대한 연체료는 미납월과 관계없이 5% (주한외국군 부대, 주한외교기관 고객에 대하여는 연체료를 적용하지 않음)청구서 배달 지연 등 아래의 경우에는납기일을 다시 지정하여 요금청구서를 재발행하여 드립니다. 다시 지정된 납기일이란 청구서를 재발급하여 고객에게 드린 날로부터 7일째 되는 날을 기준으로 정하는 것을 원칙으로 하고 있습니다.1.한전의 귀책에 의하여 송달이 지연된 경우,2. 우편물 폭주로 인하여 송달이 지연된 경우,3.기타 한전에서송달 지연을 인정하는 경우※ 연체료 면제 또는 납기일 연장된 고객이 다시 지정한 납기일까지 미납시에는 다음달 요금에 연체료가 합산 부과됨.※ 전기요금을 납기일로부터 2개월이 되는 날까지 납부하지 않는 고객에 대하여는 단전일(해지예정일) 7일전까지 미납요금 납부에 관한 사항을 안내 후 전기사용계약을 해지하므로 전기요금을 체납하지 않도록 주의하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb829c2b-e03c-4dda-b2f6-b8e06c4a5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "greedy_settings = GENSettings(max_length=350)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"전기요금을 미납한 경우 연체료율은 몇%입니까?\",\n",
    "    args=greedy_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b4f7510-8cc6-4b88-aefd-1c828668cdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전기요금을 납기일까지 납부하지 아니한 경우에는 연체료를 부담하셔야 합니다. 연체료율은 기간별로 차등적용하며 일수계산하여 부과하고 있습니다. - 전기요금 연체료 : 처음 1개월 미납시 1.5%로 일수계산,다음 1개월 미납시 1.5%로 일수계산 - TV수신료에대한 연체료는 미납월과 관계없이 5% (주한외국군 부대, 주한외교기관 고객에 대하여는 연체료를 적용하지 않음)청구서 배달 지연 등 아래의 경우에는납기일을 다시 지정하여 요금청구서를 재발행하여 드립니다. 다시 지정된 납기일이란 청구서를 재발급하여 고객에게 드린 날로부터 7일째 되는 날을 기준으로 정하는 것을 원칙으로 하고 있습니다.1.한전의 귀책에 의하여 송달이 지연된 경우,2. 우편물 폭주로 인하여 송달이 지연된 경우,3.기타 한전에서송달 지연을 인정하는 경우※ 연체료 면제 또는 납기일 연장된 고객이 다시 지정한 납기일까지 미납시에는 다음달 요금에 연체료가 합산 부과됨.※ 전기요금을 납기일로부터 2개월이 되는 날까지 납부하지 않는 고객에 대하여는 단전일(해지예정일) 7일전까지 미납요금 납부에 관한 사항을 안내 후 전기사용계약을 해지하므로 전기요금을 체납하지 않도록 주의하시기 바랍니다.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy.text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e5c93-18a7-4e35-b9ec-b9f21f9964f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_settings = GENSettings(max_length=350)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"전기요금을 미납한 경우 연체료율은 몇%입니까?\",\n",
    "    args=greedy_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8d30c-7e42-46e1-93d0-e6801121c683",
   "metadata": {},
   "source": [
    "# 정확성 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc67b0-447f-4f1c-b91f-2b8a666f1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_settings = GENSettings(max_length=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1f7fe39-40ea-4a44-a1e3-90699cd3dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 48.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\gpt\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\gpt\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\gpt\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a43ddb5-1641-44d1-9324-44aa9267bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_elec = []\n",
    "for i in range(len(elec)):\n",
    "    if len(elec.loc[i, '내용']) <= 700:\n",
    "        short_elec.append([elec.loc[i, '제목'], elec.loc[i, '내용']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8f860-b309-45b6-8a2e-dbe794f70da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "result = []\n",
    "BLEUscore_sum = []\n",
    "for i in range(len(short_elec)): # len(elec)):\n",
    "    title = short_elec[i][0].replace(\".\", \"?\")\n",
    "    output_greedy = happy_gen.generate_text(title, args=greedy_settings)\n",
    "    res = output_greedy.text.replace(\"\\n\", \"\")\n",
    "    BLEU = nltk.translate.bleu_score.sentence_bleu([short_elec[i][1]], res)  # BLEU 스코어 측정\n",
    "    BLEUscore_sum.append(BLEU) \n",
    "    result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "52db5562-6877-4f36-a009-c4fa92c0cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881187745869855"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(BLEUscore_sum) / len(short_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6d591c9-9523-42ba-864f-826a7231fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for i in range(len(short_elec)):\n",
    "    l = [short_elec[i][0], short_elec[i][1], result[i], BLEUscore_sum[i]]\n",
    "    lis.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb74eede-f0f1-41af-922e-58da5e9fbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df = pd.DataFrame(lis, columns=['질문', '답변', '예측답변', 'BLEU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b6038f5-f194-44af-99d5-24b163e7917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df.to_csv(\"elec_result.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe8c4a-17ce-41e9-8d04-5b7e470b271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7d5dcb-b365-4d8e-b0fa-6723980cc34b",
   "metadata": {},
   "source": [
    "# 평문으로 학습하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6405967a-cfa5-4705-8cd8-36d7bf11db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('esg.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9d1dbb-32c6-4ccb-94b9-3f3722f0dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.Series(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1846be-3fde-4b62-bc39-cf9f6967983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\n 없애기 \n",
    "df = [i for i in df if i != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09ec634-d999-4d95-9f7b-ab0a7d218c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c30b4f958e4db8bc0d36db7fe31a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/164 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790337eb41b544519f708b69b468ecc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de15085ad11f47b890cfe0067644eaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c3d5cece13497da05025fa00f4b740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36ff222e10d421080d53bda8dcfa906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/11.9G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30005, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\", bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\").cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e9f8a3-b890-4028-9b39-e263d5899f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetflixDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9732b743-50ad-46b5-96fd-aa128d8bbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sr = pd.Series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f240685c-51d3-4d08-9c34-0a318a4c9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for i in range(len(sr)):\n",
    "    max_length = max(max_length, len(sr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c1a672-4ad5-4675-aa4a-eab0bade1845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = NetflixDataset(df, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd2583a-e5b6-44de-b156-b24b6bb97a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 72\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n",
      "  Number of trainable parameters = 5884444672\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 23.99 GiB total capacity; 23.14 GiB already allocated; 0 bytes free; 23.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m random_split(dataset, [train_size, \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size])\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                   save_strategy\u001b[38;5;241m=\u001b[39mIntervalStrategy\u001b[38;5;241m.\u001b[39mNO,\n\u001b[0;32m      5\u001b[0m                                   per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                   warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:654\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    652\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 654\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    667\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:546\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    539\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    540\u001b[0m         create_custom_forward(layer),\n\u001b[0;32m    541\u001b[0m         hidden_states,\n\u001b[0;32m    542\u001b[0m         attention_mask,\n\u001b[0;32m    543\u001b[0m         head_mask[i],\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 546\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:319\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    311\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    317\u001b[0m ):\n\u001b[1;32m--> 319\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpt\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:141\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    139\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value, seq_len\u001b[38;5;241m=\u001b[39mseq_len)\n\u001b[0;32m    140\u001b[0m query, key \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m--> 141\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_pass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((key, key_pass), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Cache QKV values\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 23.99 GiB total capacity; 23.14 GiB already allocated; 0 bytes free; 23.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=5, logging_steps=5000,\n",
    "                                  save_strategy=IntervalStrategy.NO,\n",
    "                                  per_device_train_batch_size=2, per_device_eval_batch_size=2,\n",
    "                                  warmup_steps=100, weight_decay=0.01, logging_dir='./logs')\n",
    "Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f94d74-2c18-483c-a0c7-3be067524053",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./polyglot_ko_0324/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f493d6-08e5-4c19-aa68-b7d672b66057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532fbd3700b84aee8e493c93227ea4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d880960a-d5a2-4305-86d9-bbc6a19d397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in C:\\Users\\User\\AppData\\Local\\Temp\\tmp415kk3f0\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\User\\AppData\\Local\\Temp\\tmp415kk3f0\\special_tokens_map.json\n",
      "Uploading the following files to eunyounglee/polyglot_kr_0324: special_tokens_map.json,tokenizer.json,tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eunyounglee/polyglot_kr_0324/commit/b46f9b2cec65c6371b05a27e427df269ea30d9ad', commit_message='Upload tokenizer', commit_description='', oid='b46f9b2cec65c6371b05a27e427df269ea30d9ad', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"eunyounglee/polyglot_kr_0324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2613c352-e408-4d5a-9c76-c6b794015399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       기업들 대다수가 올해 ESG 경영 규모를 작년 수준 이상으로 유지하겠다고 밝혔다.\\n\n",
       "1     전국경제인연합회(회장 허창수)가 여론조사기관 모노리서치에 의뢰해 매출액 500대 기...\n",
       "2     전경련은 올해 ESG 사업 규모 확장의 배경을 기후 위기 극복을 위한 글로벌 공감대...\n",
       "3     E·S·G 중 가장 중요한 이슈로는 E(환경)가 82.0%로 가장 높았으며 S(사회...\n",
       "4     기업들은 NDC 2030 온실가스 감축 목표 달성을 위한 기후변화 대응전략과 구체적...\n",
       "                            ...                        \n",
       "75    앞서, 본사는 2021년 제주 고유의 맛과 특색을 살린 제주 특화 브랜드 ‘제주담음...\n",
       "76    푸드테크기업 힘난다에서는 본사에서 운영하는 수제버거 프랜차이즈 힘난다버거를 통해 프...\n",
       "77    힘난다는 최근 공식 블로그를 통해 '2023 힘나는 약속'을 선보여 가맹점에 대한 ...\n",
       "78    본사 관계자는 \"외식업계의 불황기인 만큼, 본사의 존재 이유는 전국 가맹점을 소생하...\n",
       "79    업계 관계자는 \"100년 기업으로 성장하기 위해서는 재무적인 부분만을 충족함이 아니...\n",
       "Length: 80, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1f3307d-d457-49b9-b183-e3487d379b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f271a94b6784ec099e016bde34c35cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\User/.cache\\huggingface\\hub\\models--eunyounglee--polyglot_kr_0324\\snapshots\\c5ce5b1a99c93283c83bdbab0dec500a04029442\\config.json\n",
      "Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"eunyounglee/polyglot_kr_0324\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.5,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 30005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98237e3c6104c9fbc2545ae1b38f907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\User/.cache\\huggingface\\hub\\models--eunyounglee--polyglot_kr_0324\\snapshots\\c5ce5b1a99c93283c83bdbab0dec500a04029442\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at eunyounglee/polyglot_kr_0324.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd53137ba549b5a55e39901e46faf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/381 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd69e4526ab4c79bae3b40dad6bdaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63931213e7874498bb8b40f86fc838ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at C:\\Users\\User/.cache\\huggingface\\hub\\models--eunyounglee--polyglot_kr_0324\\snapshots\\c5ce5b1a99c93283c83bdbab0dec500a04029442\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\User/.cache\\huggingface\\hub\\models--eunyounglee--polyglot_kr_0324\\snapshots\\c5ce5b1a99c93283c83bdbab0dec500a04029442\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\User/.cache\\huggingface\\hub\\models--eunyounglee--polyglot_kr_0324\\snapshots\\c5ce5b1a99c93283c83bdbab0dec500a04029442\\tokenizer_config.json\n",
      "03/24/2023 16:18:08 - INFO - happytransformer.happy_transformer -   Using model: cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyGeneration, GENSettings\n",
    "\n",
    "#---------------------------------------------------\n",
    "happy_gen = HappyGeneration(model_type=\"GPT-NEO\", model_name=\"eunyounglee/polyglot_kr_0324\")\n",
    "greedy_settings = GENSettings(max_length=300)\n",
    "output_greedy = happy_gen.generate_text(\n",
    "    \"전국경제인연합회의 회장은 누군가요?\",\n",
    "    args=greedy_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce3c9e01-11c5-4b51-9d87-784615003808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'롯데건설은 산업통상자원부 한국에너지기술평가원의 ‘시멘트 산업 발생 이산화탄소 활용 탄산화 기술개발’ 연구와 한국산업기술평가관리원의 ‘이산화탄소 반응 경화 시멘트 개발’ 연구에 공동 연구사로 참여 중이다. 탄산화 기술개발 연구는 산업 공정에서 발생한 이산화탄소를 고농도로 포집해 레미콘 공장으로 운송 후 레미콘 생산에 이용하는 기술 개발을 목적으로 한다. 시멘트 개발 연구는 이산화탄소와 반응해 굳어지는 시멘트를 개발한다.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f9d691-5eae-4add-8442-9d8a55d4b022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 기업들의 대표이사와 이사회 의장이 분리된 곳은 어디인가요.\\n 등을 중심으로 한 ESG 경영이 필수가 된 가운데, 기업들의 ESG 경영에 대한 인식과 실행력에서는 보완할 부분이 적지 않다는 평가다. 특히 매출이 높은 일부 기업 중에는 외형에 비해 ESG 경영을 소비자 친화 정책의 일환으로 접근하는 경우도 있다. 관점의 전환이 필요하다는 지적이다.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4234a219-55a5-4132-8f6c-553007d4b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 기업들은 어떤 형태로 ESG 경영을 실천하고 있나요. 사례를 통해 ESG의 실천이란 어떤 것인지 구체적으로 알아봅니다. \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy = happy_gen.generate_text(\n",
    "    \"ESG가 무엇인가요?\",\n",
    "    args=greedy_settings)\n",
    "output_greedy.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f7da6-2ca5-4b11-a183-49253feb86c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
